[32m[INFO][0m[2022-05-03 03:19:26]: [32mAlogrithm type:<class 'easyrl.configs.sac_config.SACConfig'>[0m
[32m[INFO][0m[2022-05-03 03:19:26]: [32mCreating 1 environments.[0m
pybullet build time: Mar 26 2022 03:00:15
[32m[INFO][0m[2022-05-03 03:19:26]: [32mLoad in OpenGL![0m
EGL device choice: -1 of 3.
argv[0]=
Loaded EGL 1.5 after reload.
GL_VENDOR=NVIDIA Corporation
GL_RENDERER=NVIDIA GeForce RTX 3080 Ti/PCIe/SSE2
GL_VERSION=4.6.0 NVIDIA 470.103.01
GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA
Version = 4.6.0 NVIDIA 470.103.01
Vendor = NVIDIA Corporation
Renderer = NVIDIA GeForce RTX 3080 Ti/PCIe/SSE2
b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
ee_linkb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
baseb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
tool0b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
ee_tipINFO     planner time limit: None
INFO     planner memory limit: None

INFO     Running translator.
INFO     translator stdin: None
INFO     translator time limit: None
INFO     translator memory limit: None
INFO     translator command line string: /home/wbm3/anaconda3/envs/6484_dqn/bin/python /home/wbm3/Documents/GitHub/downward/builds/release/bin/translate/translate.py /home/wbm3/Documents/GitHub/airobot_reward_densification/envs/reaching_env/single_subgoal/goal-subgoal-domain.pddl /home/wbm3/Documents/GitHub/airobot_reward_densification/envs/reaching_env/single_subgoal/goal-subgoal-problem.pddl --sas-file output.sas
Parsing...
Parsing: [0.000s CPU, 0.000s wall-clock]
Normalizing task... [0.000s CPU, 0.000s wall-clock]
Instantiating...
Generating Datalog program... [0.000s CPU, 0.000s wall-clock]
Normalizing Datalog program...
Normalizing Datalog program: [0.000s CPU, 0.000s wall-clock]
Preparing model... [0.000s CPU, 0.000s wall-clock]
Generated 10 rules.
Computing model... [0.000s CPU, 0.000s wall-clock]
17 relevant atoms
8 auxiliary atoms
25 final queue length
27 total queue pushes
Completing instantiation... [0.000s CPU, 0.000s wall-clock]
Instantiating: [0.000s CPU, 0.001s wall-clock]
Computing fact groups...
Finding invariants...
3 initial candidates
Finding invariants: [0.000s CPU, 0.000s wall-clock]
Checking invariant weight... [0.000s CPU, 0.000s wall-clock]
Instantiating groups... [0.000s CPU, 0.000s wall-clock]
Collecting mutex groups... [0.010s CPU, 0.000s wall-clock]
Choosing groups...
2 uncovered facts
Choosing groups: [0.000s CPU, 0.000s wall-clock]
Building translation key... [0.000s CPU, 0.000s wall-clock]
Computing fact groups: [0.010s CPU, 0.000s wall-clock]
Building STRIPS to SAS dictionary... [0.000s CPU, 0.000s wall-clock]
Building dictionary for full mutex groups... [0.000s CPU, 0.000s wall-clock]
Building mutex information...
Building mutex information: [0.000s CPU, 0.000s wall-clock]
Translating task...
Processing axioms...
Simplifying axioms... [0.000s CPU, 0.000s wall-clock]
Translator axioms removed by simplifying: 0
Computing negative axioms... [0.000s CPU, 0.000s wall-clock]
Processing axioms: [0.000s CPU, 0.000s wall-clock]
Translating task: [0.000s CPU, 0.000s wall-clock]
1 effect conditions simplified
0 implied preconditions added
Detecting unreachable propositions...
0 operators removed
0 axioms removed
0 propositions removed
Detecting unreachable propositions: [0.000s CPU, 0.000s wall-clock]
Reordering and filtering variables...
2 of 2 variables necessary.
0 of 0 mutex groups necessary.
2 of 2 operators necessary.
0 of 0 axiom rules necessary.
Reordering and filtering variables: [0.000s CPU, 0.000s wall-clock]
Translator variables: 2
Translator derived variables: 0
Translator facts: 4
Translator goal facts: 1
Translator mutex groups: 0
Translator total mutex groups size: 0
Translator operators: 2
Translator axioms: 0
Translator task size: 15
Translator peak memory: 29696 KB
Writing output... [0.000s CPU, 0.000s wall-clock]
Done! [0.010s CPU, 0.002s wall-clock]
translate exit code: 0

INFO     Running search (release).
INFO     search stdin: output.sas
INFO     search time limit: None
INFO     search memory limit: None
INFO     search command line string: /home/wbm3/Documents/GitHub/downward/builds/release/bin/downward --if-unit-cost --evaluator 'hlm=lmcount(lm_reasonable_orders_hps(lm_rhw()),pref=true)' --evaluator 'hff=ff()' --search 'iterated([lazy_greedy([hff,hlm],preferred=[hff,hlm]),lazy_wastar([hff,hlm],preferred=[hff,hlm],w=5),lazy_wastar([hff,hlm],preferred=[hff,hlm],w=3),lazy_wastar([hff,hlm],preferred=[hff,hlm],w=2),lazy_wastar([hff,hlm],preferred=[hff,hlm],w=1)],repeat_last=true,continue_on_fail=true)' --if-non-unit-cost --evaluator 'hlm1=lmcount(lm_reasonable_orders_hps(lm_rhw()),transform=adapt_costs(one),pref=true)' --evaluator 'hff1=ff(transform=adapt_costs(one))' --evaluator 'hlm2=lmcount(lm_reasonable_orders_hps(lm_rhw()),transform=adapt_costs(plusone),pref=true)' --evaluator 'hff2=ff(transform=adapt_costs(plusone))' --search 'iterated([lazy_greedy([hff1,hlm1],preferred=[hff1,hlm1],cost_type=one,reopen_closed=false),lazy_greedy([hff2,hlm2],preferred=[hff2,hlm2],reopen_closed=false),lazy_wastar([hff2,hlm2],preferred=[hff2,hlm2],w=5),lazy_wastar([hff2,hlm2],preferred=[hff2,hlm2],w=3),lazy_wastar([hff2,hlm2],preferred=[hff2,hlm2],w=2),lazy_wastar([hff2,hlm2],preferred=[hff2,hlm2],w=1)],repeat_last=true,continue_on_fail=true)' --always --internal-plan-file sas_plan < output.sas
[t=1.62e-05s, 13696 KB] reading input...
[t=0.00011003s, 13696 KB] done reading input!
[t=0.00125899s, 14084 KB] Initializing landmark count heuristic...
[t=0.00129977s, 14084 KB] Generating landmark graph...
[t=0.00131843s, 14084 KB] Building a landmark graph with reasonable orders.
[t=0.00133226s, 14084 KB] Initializing Exploration...
[t=0.00134708s, 14084 KB] Generating landmarks using the RPG/SAS+ approach
[t=0.00138519s, 14084 KB] Removed 0 reasonable or obedient reasonable orders
[t=0.00139915s, 14084 KB] Landmarks generation time: 7.891e-05s
[t=0.00141221s, 14084 KB] Discovered 4 landmarks, of which 0 are disjunctive and 0 are conjunctive.
[t=0.00142357s, 14084 KB] 3 edges
[t=0.00143505s, 14084 KB] approx. reasonable orders
[t=0.00144846s, 14084 KB] approx. obedient reasonable orders
[t=0.00146052s, 14084 KB] Removed 0 reasonable or obedient reasonable orders
[t=0.0014715s, 14084 KB] Landmarks generation time: 0.00016464s
[t=0.00148308s, 14084 KB] Discovered 4 landmarks, of which 0 are disjunctive and 0 are conjunctive.
[t=0.00149399s, 14084 KB] 3 edges
[t=0.00150468s, 14084 KB] Landmark graph generation time: 0.0002148s
[t=0.00151611s, 14084 KB] Landmark graph contains 4 landmarks, of which 0 are disjunctive and 0 are conjunctive.
[t=0.001527s, 14084 KB] Landmark graph contains 3 orderings.
[t=0.00156489s, 14084 KB] Simplifying 3 unary operators... done! [3 unary operators]
[t=0.00158561s, 14084 KB] time to simplify: 3.207e-05s
[t=0.00159925s, 14084 KB] Initializing additive heuristic...
[t=0.00161014s, 14084 KB] Initializing FF heuristic...
[t=0.00168604s, 14084 KB] Building successor generator...done!
[t=0.00172816s, 14084 KB] peak memory difference for successor generator creation: 0 KB
[t=0.00173918s, 14084 KB] time for successor generation creation: 3.36e-06s
[t=0.00175231s, 14084 KB] Variables: 2
[t=0.00176326s, 14084 KB] FactPairs: 4
[t=0.00177394s, 14084 KB] Bytes per state: 4
[t=0.00203001s, 14348 KB] Building successor generator...done!
[t=0.00209804s, 14348 KB] peak memory difference for successor generator creation: 0 KB
[t=0.00211446s, 14348 KB] time for successor generation creation: 4.7e-07s
[t=0.00212823s, 14348 KB] Variables: 2
[t=0.00213949s, 14348 KB] FactPairs: 4
[t=0.00215027s, 14348 KB] Bytes per state: 4
[t=0.00217208s, 14348 KB] Starting search: lazy_greedy(list(hff, hlm), preferred = list(hff, hlm))
[t=0.00218707s, 14348 KB] Conducting lazy best first search, (real) bound = 2147483647
[t=0.002203s, 14348 KB] 2 initial landmarks, 1 goal landmarks
[t=0.00223014s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00224425s, 14348 KB] New best heuristic value for ff: 2
[t=0.0022558s, 14348 KB] g=0, 1 evaluated, 0 expanded
[t=0.00226998s, 14348 KB] Initial heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00228102s, 14348 KB] Initial heuristic value for ff: 2
[t=0.00229766s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 1
[t=0.00230957s, 14348 KB] New best heuristic value for ff: 1
[t=0.00232043s, 14348 KB] g=1, 2 evaluated, 1 expanded
[t=0.00233547s, 14348 KB] Solution found!
[t=0.00234774s, 14348 KB] Actual search time: 0.00013953s
move-to-subgoal claw subgoal (1)
move-to-goal claw subgoal goal (1)
[t=0.00239028s, 14348 KB] Plan length: 2 step(s).
[t=0.00240545s, 14348 KB] Plan cost: 2
[t=0.0024173s, 14348 KB] Expanded 2 state(s).
[t=0.0024281s, 14348 KB] Reopened 0 state(s).
[t=0.00243873s, 14348 KB] Evaluated 3 state(s).
[t=0.0024493s, 14348 KB] Evaluations: 6
[t=0.00246031s, 14348 KB] Generated 2 state(s).
[t=0.00247103s, 14348 KB] Dead ends: 0 state(s).
[t=0.00248178s, 14348 KB] Number of registered states: 3
[t=0.00249256s, 14348 KB] Int hash set load factor: 3/4 = 0.75
[t=0.00250392s, 14348 KB] Int hash set resizes: 2
[t=0.00251467s, 14348 KB] Best solution cost so far: 2
[t=0.00252521s, 14348 KB] Solution found - keep searching
[t=0.00258312s, 14348 KB] Building successor generator...done!
[t=0.00262017s, 14348 KB] peak memory difference for successor generator creation: 0 KB
[t=0.00263095s, 14348 KB] time for successor generation creation: 3.8e-07s
[t=0.00264307s, 14348 KB] Variables: 2
[t=0.00265382s, 14348 KB] FactPairs: 4
[t=0.0026645s, 14348 KB] Bytes per state: 4
[t=0.00268159s, 14348 KB] Starting search: lazy_wastar(list(hff, hlm), preferred = list(hff, hlm), w = 5)
[t=0.00269594s, 14348 KB] Conducting lazy best first search, (real) bound = 2
[t=0.00271044s, 14348 KB] 2 initial landmarks, 1 goal landmarks
[t=0.00273002s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00274291s, 14348 KB] New best heuristic value for ff: 2
[t=0.00275419s, 14348 KB] g=0, 1 evaluated, 0 expanded
[t=0.00277297s, 14348 KB] Initial heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00278459s, 14348 KB] Initial heuristic value for ff: 2
[t=0.00280094s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 1
[t=0.00281308s, 14348 KB] New best heuristic value for ff: 1
[t=0.00282397s, 14348 KB] g=1, 2 evaluated, 1 expanded
[t=0.00283724s, 14348 KB] Completely explored state space -- no solution!
[t=0.00284835s, 14348 KB] Actual search time: 0.000136421s
[t=0.00286106s, 14348 KB] Expanded 2 state(s).
[t=0.00287213s, 14348 KB] Reopened 0 state(s).
[t=0.00288268s, 14348 KB] Evaluated 2 state(s).
[t=0.00289357s, 14348 KB] Evaluations: 4
[t=0.00290426s, 14348 KB] Generated 2 state(s).
[t=0.0029149s, 14348 KB] Dead ends: 0 state(s).
[t=0.00292548s, 14348 KB] Number of registered states: 2
[t=0.00293613s, 14348 KB] Int hash set load factor: 2/2 = 1
[t=0.00294756s, 14348 KB] Int hash set resizes: 1
[t=0.00295833s, 14348 KB] Best solution cost so far: 2
[t=0.00296914s, 14348 KB] No solution found - keep searching
[t=0.00302026s, 14348 KB] Building successor generator...done!
[t=0.00305548s, 14348 KB] peak memory difference for successor generator creation: 0 KB
[t=0.00306704s, 14348 KB] time for successor generation creation: 3.8e-07s
[t=0.00307903s, 14348 KB] Variables: 2
[t=0.00308982s, 14348 KB] FactPairs: 4
[t=0.00310459s, 14348 KB] Bytes per state: 4
[t=0.00312239s, 14348 KB] Starting search: lazy_wastar(list(hff, hlm), preferred = list(hff, hlm), w = 3)
[t=0.00313594s, 14348 KB] Conducting lazy best first search, (real) bound = 2
[t=0.00314966s, 14348 KB] 2 initial landmarks, 1 goal landmarks
[t=0.00316761s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00317929s, 14348 KB] New best heuristic value for ff: 2
[t=0.00319084s, 14348 KB] g=0, 1 evaluated, 0 expanded
[t=0.00320483s, 14348 KB] Initial heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00321597s, 14348 KB] Initial heuristic value for ff: 2
[t=0.00323191s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 1
[t=0.00324407s, 14348 KB] New best heuristic value for ff: 1
[t=0.00325496s, 14348 KB] g=1, 2 evaluated, 1 expanded
[t=0.00326773s, 14348 KB] Completely explored state space -- no solution!
[t=0.00327871s, 14348 KB] Actual search time: 0.00012697s
[t=0.0032904s, 14348 KB] Expanded 2 state(s).
[t=0.00330124s, 14348 KB] Reopened 0 state(s).
[t=0.00331189s, 14348 KB] Evaluated 2 state(s).
[t=0.00332269s, 14348 KB] Evaluations: 4
[t=0.00333328s, 14348 KB] Generated 2 state(s).
[t=0.00334392s, 14348 KB] Dead ends: 0 state(s).
[t=0.00335454s, 14348 KB] Number of registered states: 2
[t=0.00336518s, 14348 KB] Int hash set load factor: 2/2 = 1
[t=0.00337646s, 14348 KB] Int hash set resizes: 1
[t=0.00338698s, 14348 KB] Best solution cost so far: 2
[t=0.00339747s, 14348 KB] No solution found - keep searching
[t=0.00344665s, 14348 KB] Building successor generator...done!
[t=0.00348132s, 14348 KB] peak memory difference for successor generator creation: 0 KB
[t=0.00349193s, 14348 KB] time for successor generation creation: 3.4e-07s
[t=0.00350411s, 14348 KB] Variables: 2
[t=0.0035151s, 14348 KB] FactPairs: 4
[t=0.00352585s, 14348 KB] Bytes per state: 4
[t=0.00354216s, 14348 KB] Starting search: lazy_wastar(list(hff, hlm), preferred = list(hff, hlm), w = 2)
[t=0.00355519s, 14348 KB] Conducting lazy best first search, (real) bound = 2
[t=0.00356848s, 14348 KB] 2 initial landmarks, 1 goal landmarks
[t=0.00358675s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00359854s, 14348 KB] New best heuristic value for ff: 2
[t=0.0036098s, 14348 KB] g=0, 1 evaluated, 0 expanded
[t=0.00362348s, 14348 KB] Initial heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00363448s, 14348 KB] Initial heuristic value for ff: 2
[t=0.00365003s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 1
[t=0.00366185s, 14348 KB] New best heuristic value for ff: 1
[t=0.00367292s, 14348 KB] g=1, 2 evaluated, 1 expanded
[t=0.00368555s, 14348 KB] Completely explored state space -- no solution!
[t=0.00369657s, 14348 KB] Actual search time: 0.00012563s
[t=0.00370847s, 14348 KB] Expanded 2 state(s).
[t=0.00371903s, 14348 KB] Reopened 0 state(s).
[t=0.00372957s, 14348 KB] Evaluated 2 state(s).
[t=0.00374016s, 14348 KB] Evaluations: 4
[t=0.00375071s, 14348 KB] Generated 2 state(s).
[t=0.00376132s, 14348 KB] Dead ends: 0 state(s).
[t=0.0037721s, 14348 KB] Number of registered states: 2
[t=0.0037827s, 14348 KB] Int hash set load factor: 2/2 = 1
[t=0.00379393s, 14348 KB] Int hash set resizes: 1
[t=0.00380458s, 14348 KB] Best solution cost so far: 2
[t=0.00381517s, 14348 KB] No solution found - keep searching
[t=0.00386238s, 14348 KB] Building successor generator...done!
[t=0.00389662s, 14348 KB] peak memory difference for successor generator creation: 0 KB
[t=0.00390732s, 14348 KB] time for successor generation creation: 3.7e-07s
[t=0.00391987s, 14348 KB] Variables: 2
[t=0.00393075s, 14348 KB] FactPairs: 4
[t=0.00394145s, 14348 KB] Bytes per state: 4
[t=0.00395793s, 14348 KB] Starting search: lazy_wastar(list(hff, hlm), preferred = list(hff, hlm), w = 1)
[t=0.00397092s, 14348 KB] Conducting lazy best first search, (real) bound = 2
[t=0.00398961s, 14348 KB] 2 initial landmarks, 1 goal landmarks
[t=0.00400788s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00402019s, 14348 KB] New best heuristic value for ff: 2
[t=0.00403117s, 14348 KB] g=0, 1 evaluated, 0 expanded
[t=0.00404484s, 14348 KB] Initial heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 2
[t=0.00405593s, 14348 KB] Initial heuristic value for ff: 2
[t=0.00407141s, 14348 KB] New best heuristic value for lmcount(lm_reasonable_orders_hps(lm_rhw), pref = true): 1
[t=0.00408341s, 14348 KB] New best heuristic value for ff: 1
[t=0.00409421s, 14348 KB] g=1, 2 evaluated, 1 expanded
[t=0.00410683s, 14348 KB] Completely explored state space -- no solution!
[t=0.00411798s, 14348 KB] Actual search time: 0.00012576s
[t=0.00412987s, 14348 KB] Expanded 2 state(s).
[t=0.00414079s, 14348 KB] Reopened 0 state(s).
[t=0.00415154s, 14348 KB] Evaluated 2 state(s).
[t=0.00416205s, 14348 KB] Evaluations: 4
[t=0.00417243s, 14348 KB] Generated 2 state(s).
[t=0.00418296s, 14348 KB] Dead ends: 0 state(s).
[t=0.00419384s, 14348 KB] Number of registered states: 2
[t=0.00420447s, 14348 KB] Int hash set load factor: 2/2 = 1
[t=0.00421588s, 14348 KB] Int hash set resizes: 1
[t=0.00422664s, 14348 KB] Best solution cost so far: 2
[t=0.00423733s, 14348 KB] No solution found - keep searching
[t=0.00425052s, 14348 KB] Actual search time: 0.00226789s
[t=0.00426346s, 14348 KB] Cumulative statistics:
[t=0.00426346s, 14348 KB] Expanded 10 state(s).
[t=0.00426346s, 14348 KB] Reopened 0 state(s).
[t=0.00426346s, 14348 KB] Evaluated 11 state(s).
[t=0.00426346s, 14348 KB] Evaluations: 22
[t=0.00426346s, 14348 KB] Generated 10 state(s).
[t=0.00426346s, 14348 KB] Dead ends: 0 state(s).
[t=0.00426346s, 14348 KB] Search time: 0.00227058s
[t=0.00426346s, 14348 KB] Total time: 0.00426346s
Solution found.
Peak memory: 14348 KB
Remove intermediate file output.sas
search exit code: 0

INFO     Planner time: 0.04s
/home/wbm3/anaconda3/envs/6484_dqn/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32m[INFO][0m[2022-05-03 03:19:39]: [32mExploration steps: 10050[0m
[32m[INFO][0m[2022-05-03 03:19:39]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000010050.pt.[0m
[32m[INFO][0m[2022-05-03 03:19:39]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/model_best.pt.[0m
[32m[INFO][0m[2022-05-03 03:19:39]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:19:39]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:20:07]: [32mExploration steps: 15050[0m
[32m[INFO][0m[2022-05-03 03:20:07]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000015050.pt.[0m
[32m[INFO][0m[2022-05-03 03:20:07]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:20:07]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:20:35]: [32mExploration steps: 20050[0m
[32m[INFO][0m[2022-05-03 03:20:35]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000020050.pt.[0m
[32m[INFO][0m[2022-05-03 03:20:35]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:20:35]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:21:02]: [32mExploration steps: 25050[0m
[32m[INFO][0m[2022-05-03 03:21:02]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000025050.pt.[0m
[32m[INFO][0m[2022-05-03 03:21:02]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:21:03]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:21:29]: [32mExploration steps: 30050[0m
[32m[INFO][0m[2022-05-03 03:21:29]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000030050.pt.[0m
[32m[INFO][0m[2022-05-03 03:21:29]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:21:29]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:21:56]: [32mExploration steps: 35050[0m
[32m[INFO][0m[2022-05-03 03:21:56]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000035050.pt.[0m
[32m[INFO][0m[2022-05-03 03:21:56]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:21:56]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:22:23]: [32mExploration steps: 40050[0m
[32m[INFO][0m[2022-05-03 03:22:23]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000040050.pt.[0m
[32m[INFO][0m[2022-05-03 03:22:23]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:22:23]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:22:50]: [32mExploration steps: 45050[0m
[32m[INFO][0m[2022-05-03 03:22:50]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000045050.pt.[0m
[32m[INFO][0m[2022-05-03 03:22:50]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:22:51]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:23:17]: [32mExploration steps: 50050[0m
[32m[INFO][0m[2022-05-03 03:23:17]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000050050.pt.[0m
[32m[INFO][0m[2022-05-03 03:23:18]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:23:18]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:23:44]: [32mExploration steps: 55050[0m
[32m[INFO][0m[2022-05-03 03:23:44]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000055050.pt.[0m
[32m[INFO][0m[2022-05-03 03:23:44]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:23:45]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:24:11]: [32mExploration steps: 60050[0m
[32m[INFO][0m[2022-05-03 03:24:11]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000060050.pt.[0m
[32m[INFO][0m[2022-05-03 03:24:11]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:24:12]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:24:38]: [32mExploration steps: 65050[0m
[32m[INFO][0m[2022-05-03 03:24:38]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000065050.pt.[0m
[32m[INFO][0m[2022-05-03 03:24:38]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:24:39]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:25:06]: [32mExploration steps: 70050[0m
[32m[INFO][0m[2022-05-03 03:25:06]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000070050.pt.[0m
[32m[INFO][0m[2022-05-03 03:25:06]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:25:06]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:25:33]: [32mExploration steps: 75050[0m
[32m[INFO][0m[2022-05-03 03:25:33]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000075050.pt.[0m
[32m[INFO][0m[2022-05-03 03:25:33]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:25:34]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:26:00]: [32mExploration steps: 80050[0m
[32m[INFO][0m[2022-05-03 03:26:00]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000080050.pt.[0m
[32m[INFO][0m[2022-05-03 03:26:00]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:26:01]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:26:28]: [32mExploration steps: 85050[0m
[32m[INFO][0m[2022-05-03 03:26:28]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000085050.pt.[0m
[32m[INFO][0m[2022-05-03 03:26:28]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:26:29]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:26:58]: [32mExploration steps: 90050[0m
[32m[INFO][0m[2022-05-03 03:26:58]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000090050.pt.[0m
[32m[INFO][0m[2022-05-03 03:26:58]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:26:59]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:27:27]: [32mExploration steps: 95050[0m
[32m[INFO][0m[2022-05-03 03:27:27]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000095050.pt.[0m
[32m[INFO][0m[2022-05-03 03:27:27]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:27:28]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:27:56]: [32mExploration steps: 100050[0m
[32m[INFO][0m[2022-05-03 03:27:56]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000100050.pt.[0m
[32m[INFO][0m[2022-05-03 03:27:56]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:27:57]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:28:26]: [32mExploration steps: 105050[0m
[32m[INFO][0m[2022-05-03 03:28:26]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000105050.pt.[0m
[32m[INFO][0m[2022-05-03 03:28:26]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:28:27]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:28:56]: [32mExploration steps: 110050[0m
[32m[INFO][0m[2022-05-03 03:28:56]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000110050.pt.[0m
[32m[INFO][0m[2022-05-03 03:28:56]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:28:57]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:29:25]: [32mExploration steps: 115050[0m
[32m[INFO][0m[2022-05-03 03:29:25]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000115050.pt.[0m
[32m[INFO][0m[2022-05-03 03:29:25]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:29:26]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:29:53]: [32mExploration steps: 120050[0m
[32m[INFO][0m[2022-05-03 03:29:53]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000120050.pt.[0m
[32m[INFO][0m[2022-05-03 03:29:53]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:29:54]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:30:21]: [32mExploration steps: 125050[0m
[32m[INFO][0m[2022-05-03 03:30:21]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000125050.pt.[0m
[32m[INFO][0m[2022-05-03 03:30:21]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:30:23]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:30:50]: [32mExploration steps: 130050[0m
[32m[INFO][0m[2022-05-03 03:30:50]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000130050.pt.[0m
[32m[INFO][0m[2022-05-03 03:30:50]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:30:52]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:31:19]: [32mExploration steps: 135050[0m
[32m[INFO][0m[2022-05-03 03:31:19]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000135050.pt.[0m
[32m[INFO][0m[2022-05-03 03:31:19]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:31:21]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:31:48]: [32mExploration steps: 140050[0m
[32m[INFO][0m[2022-05-03 03:31:48]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000140050.pt.[0m
[32m[INFO][0m[2022-05-03 03:31:48]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:31:50]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:32:17]: [32mExploration steps: 145050[0m
[32m[INFO][0m[2022-05-03 03:32:17]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000145050.pt.[0m
[32m[INFO][0m[2022-05-03 03:32:17]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:32:18]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:32:46]: [32mExploration steps: 150050[0m
[32m[INFO][0m[2022-05-03 03:32:46]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000150050.pt.[0m
[32m[INFO][0m[2022-05-03 03:32:47]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:32:48]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:33:15]: [32mExploration steps: 155050[0m
[32m[INFO][0m[2022-05-03 03:33:15]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000155050.pt.[0m
[32m[INFO][0m[2022-05-03 03:33:15]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:33:16]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:33:43]: [32mExploration steps: 160050[0m
[32m[INFO][0m[2022-05-03 03:33:43]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000160050.pt.[0m
[32m[INFO][0m[2022-05-03 03:33:43]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:33:45]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:34:12]: [32mExploration steps: 165050[0m
[32m[INFO][0m[2022-05-03 03:34:12]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000165050.pt.[0m
[32m[INFO][0m[2022-05-03 03:34:12]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:34:14]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:34:40]: [32mExploration steps: 170050[0m
[32m[INFO][0m[2022-05-03 03:34:40]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000170050.pt.[0m
[32m[INFO][0m[2022-05-03 03:34:40]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:34:42]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:35:09]: [32mExploration steps: 175050[0m
[32m[INFO][0m[2022-05-03 03:35:09]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000175050.pt.[0m
[32m[INFO][0m[2022-05-03 03:35:09]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:35:10]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:35:37]: [32mExploration steps: 180050[0m
[32m[INFO][0m[2022-05-03 03:35:37]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000180050.pt.[0m
[32m[INFO][0m[2022-05-03 03:35:37]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:35:39]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:36:06]: [32mExploration steps: 185050[0m
[32m[INFO][0m[2022-05-03 03:36:06]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000185050.pt.[0m
[32m[INFO][0m[2022-05-03 03:36:06]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:36:08]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:36:35]: [32mExploration steps: 190050[0m
[32m[INFO][0m[2022-05-03 03:36:35]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000190050.pt.[0m
[32m[INFO][0m[2022-05-03 03:36:35]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:36:36]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:37:04]: [32mExploration steps: 195050[0m
[32m[INFO][0m[2022-05-03 03:37:04]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000195050.pt.[0m
[32m[INFO][0m[2022-05-03 03:37:04]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:37:06]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:37:32]: [32mExploration steps: 200050[0m
[32m[INFO][0m[2022-05-03 03:37:32]: [32mSaving checkpoint: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/ckpt_000000200050.pt.[0m
[32m[INFO][0m[2022-05-03 03:37:32]: [32mSaving the replay buffer to: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
[32m[INFO][0m[2022-05-03 03:37:35]: [32mThe replay buffer is saved.[0m
[32m[INFO][0m[2022-05-03 03:37:35]: [32mLoading model from /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/model_best.pt[0m
[32m[INFO][0m[2022-05-03 03:37:35]: [32mLoading the replay buffer from: /home/wbm3/Documents/GitHub/airobot_reward_densification/data/URReacher-v1_reach_sparse_handcrafted_sac/seed_0/model/mem.pkl.[0m
====================================
      Device:cuda
      Total number of steps:200000
====================================
INFO: Making new env: URReacher-v1 ({'reward_type': 'sparse_handcrafted', 'gui': False, 'with_obstacle': True, 'max_episode_length': 50})
================================================
With obstacle in the scene:True
================================================
{'eval/episode_length/max': 50,
 'eval/episode_length/mean': 50.0,
 'eval/episode_length/median': 50.0,
 'eval/episode_length/min': 50,
 'eval/return/max': 0.0,
 'eval/return/mean': 0.0,
 'eval/return/median': 0.0,
 'eval/return/min': 0.0,
 'eval/smooth_return/mean': 0.0}
ven = NVIDIA Corporation
ven = NVIDIA Corporation
Destroy EGL OpenGL window.
